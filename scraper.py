# scraper.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import logging

from config import NOOR_BOOK_BASE_URL, NOOR_BOOK_SEARCH_URL, MAX_SEARCH_RESULTS

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def __init__(self):
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± Ø§Ù„Ø®Ø§Ø¯Ù…
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def search_library(self, query):
        """ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø¯Ø¯Ø§Øª CSS Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø©."""
        encoded_query = quote(query)
        search_url = NOOR_BOOK_SEARCH_URL.format(query=encoded_query)
        
        logging.info(f"Searching: {search_url}")
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'lxml')
            results = []
            
            # ğŸ’¡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¨ØªÙƒØ±: Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ø­Ø§ÙˆÙŠØ§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© (Container)
            # Ù†Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ø§Ø³Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ÙƒØªØ¨ (ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¨Ø¯Ø§Ø¦Ù„)
            book_containers = soup.select('.book-card-item, .book-card, .book-item') 
            
            if not book_containers:
                logging.warning("No book containers found using common selectors.")
            
            for container in book_containers[:MAX_SEARCH_RESULTS]:
                
                # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· Ø§Ù„ÙƒØªØ§Ø¨ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ø§ÙˆÙŠØ© (Ø±Ø§Ø¨Ø· ØµÙØ­Ø© Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©)
                # Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø£ÙŠ Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ø§ÙˆÙŠØ© ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØµÙØ­Ø© ÙƒØªØ§Ø¨ Ù…Ø­Ø¯Ø¯Ø© (URL path contains /book-)
                book_link_element = container.select_one('a[href*="/book-"]')
                
                if book_link_element:
                    book_link_partial = book_link_element.get('href')
                    book_full_link = urljoin(NOOR_BOOK_BASE_URL, book_link_partial)
                    
                    # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ù„Ø§ØµÙ‡ Ù…Ù† Ø¹Ø¯Ø© Ø£Ù…Ø§ÙƒÙ† Ø´Ø§Ø¦Ø¹Ø©)
                    title_element = container.select_one('.book-card-title, h3 a, h4 a, .book-title')
                    
                    # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¹Ù†ÙˆØ§Ù†Ø§Ù‹ Ù…Ø­Ø¯Ø¯Ø§Ù‹ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ Ø¯Ø§Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¹Ù†ØµØ±
                    book_title = title_element.text.strip() if title_element else book_link_element.text.strip()
                    
                    if book_title and book_full_link:
                        results.append({
                            'title': book_title,
                            'url': book_full_link
                        })
            
            return results

        except Exception as e:
            logging.error(f"Error during scraping search: {e}")
            return []

    def get_download_info(self, book_url):
        """ØªØ³ØªØ®Ø±Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø¯Ø¯Ø§Øª Ø£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©."""
        logging.info(f"Visiting book page: {book_url}")
        
        try:
            response = requests.get(book_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'lxml')
            
            # ğŸ’¡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¨ØªÙƒØ±: Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
            # Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø§Ù„Ø´Ø§Ù…Ù„ ÙŠØºØ·ÙŠ Ù…Ø¹Ø¸Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
            download_button = soup.select_one(
                'a[href$=".pdf"], '       # Ø±Ø§Ø¨Ø· ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ .pdf
                'a[href$=".epub"], '      # Ø±Ø§Ø¨Ø· ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ .epub
                'a[download], '           # ÙˆØ³Ù… ÙŠØ­Ù…Ù„ Ø®Ø§ØµÙŠØ© download (Ù‚ÙŠØ§Ø³ÙŠ)
                'a.btn-download, '        # ÙƒÙ„Ø§Ø³ Ø´Ø§Ø¦Ø¹ Ù„Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
                'a[href*="/download/"]'   # Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± /download/
            )
            
            if download_button:
                download_link_partial = download_button.get('href')
                download_link = urljoin(NOOR_BOOK_BASE_URL, download_link_partial)
                
                # ØªØ®Ù…ÙŠÙ† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
                file_ext = '.pdf' if '.pdf' in download_link.lower() else '.epub'
                
                return download_link, file_ext
            
            return None, None

        except Exception as e:
            logging.error(f"Error during download link extraction: {e}")
            return None, None
