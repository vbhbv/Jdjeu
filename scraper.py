# scraper.py (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ÙƒØ´Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙˆØ§Ù„ØªÙØ§ÙˆØ¶ Ø§Ù„Ø¢Ù„ÙŠ)
import logging
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote, urlparse
from config import MAX_SEARCH_RESULTS, NOOR_BOOK_BASE_URL

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def __init__(self):
        # Ø±Ø¤ÙˆØ³ Ø«Ø§Ø¨ØªØ© Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ù…ØªØµÙØ­ Chrome
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ar-EG,ar;q=0.9,en-US;q=0.8,en;q=0.7'
        }

    def search_library(self, query):
        """
        Ø§Ù„ÙƒØ´Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: ÙŠØ¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø«Ø§Ø¨ØªØ© (/book-). (ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©)
        """
        encoded_query = quote(query)
        # ğŸ’¡ ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ NOOR_BOOK_SEARCH_URL Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø®Ø·Ø£
        search_url = NOOR_BOOK_BASE_URL + "/search?query=" + encoded_query 
        
        logging.info(f"Direct Scraping Search: {search_url}")
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'lxml')
            results = []
            
            # Ù…Ø­Ø¯Ø¯ CSS Ø«Ø§Ø¨Øª: Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØªØ§Ø¨ 
            book_links = soup.select('a[href*="/book-"]')
            
            unique_books = {}
            for link in book_links:
                book_link_partial = link.get('href')
                
                if book_link_partial and book_link_partial not in unique_books:
                    book_full_link = urljoin(NOOR_BOOK_BASE_URL, book_link_partial)
                    book_title = link.get('title', link.text).strip()
                    
                    if len(book_title) > 5 and book_title.lower() not in ['details', 'read more']:
                        unique_books[book_link_partial] = {
                            'title': book_title,
                            'url': book_full_link
                        }

            results = list(unique_books.values())[:MAX_SEARCH_RESULTS]
            
            return results

        except Exception as e:
            logging.error(f"Error during direct scraping search: {e}")
            return []

    def get_download_info(self, book_url):
        """
        Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙØ§ÙˆØ¶ Ø§Ù„Ø¢Ù„ÙŠ Ù„ÙÙƒ ØªØ´ÙÙŠØ± Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ù…Ø­ØµÙ† Ø¶Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡).
        """
        logging.info(f"Attempting Automated Negotiation for download link: {book_url}")
        
        # ØªØ­Ø¯ÙŠØ« Ø±Ø£Ø³ Referer Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹
        referer_domain = urlparse(book_url).scheme + "://" + urlparse(book_url).netloc
        current_headers = self.headers.copy()
        current_headers['Referer'] = referer_domain
        
        try:
            # 1. Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ§Ø¨
            response = requests.get(book_url, headers=current_headers, timeout=15, allow_redirects=True)
            
            if response.status_code >= 400:
                logging.warning(f"Initial book page request failed with status: {response.status_code}")
                return None, "error"
                
            soup = BeautifulSoup(response.content, 'lxml')
            
            # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
            download_button = soup.select_one('a[href*="/download/"], a.btn-download, a[download], button')
            
            if download_button:
                download_link_partial = download_button.get('href') or download_button.get('data-href')

                if download_link_partial:
                    full_download_link = urljoin(response.url, download_link_partial)

                    # 3. Ø§Ù„ØªÙØ§ÙˆØ¶ Ø§Ù„Ø¢Ù„ÙŠ
                    negotiation_headers = self.headers.copy()
                    negotiation_headers['Referer'] = response.url

                    final_file_response = requests.get(
                        full_download_link, 
                        headers=negotiation_headers, 
                        timeout=30, 
                        allow_redirects=True
                    )
                    
                    final_url = final_file_response.url 
                    
                    # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ 
                    if final_url.lower().endswith(('.pdf', '.epub')):
                        file_ext = '.pdf' if final_url.lower().endswith('.pdf') else '.epub'
                        logging.info(f"Success! Found direct file link: {final_url}")
                        return final_url, file_ext
                
            return None, "link"
            
        except Exception as e:
            logging.error(f"Critical error during Automated Negotiation: {e}")
            return None, "error"
