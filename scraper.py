# scraper.py (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… requests_html)
from requests_html import HTMLSession # ğŸ’¡ ØªØºÙŠÙŠØ± Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup 
import logging

from config import NOOR_BOOK_BASE_URL, NOOR_BOOK_SEARCH_URL, MAX_SEARCH_RESULTS

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def __init__(self):
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… HTMLSession Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù€ DOM ÙˆØªÙ†ÙÙŠØ° JS
        self.session = HTMLSession()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ar-EG,ar;q=0.9,en-US;q=0.8,en;q=0.7'
        }

    def search_library(self, query):
        """ÙŠØ¨Ø­Ø« ÙˆÙŠÙ†ÙØ° JavaScript Ù„Ù„ØµÙØ­Ø© Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬."""
        encoded_query = quote(query)
        search_url = NOOR_BOOK_SEARCH_URL.format(query=encoded_query)
        
        logging.info(f"Searching and Rendering: {search_url}")
        
        try:
            # 1. Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
            response = self.session.get(search_url, headers=self.headers, timeout=20)
            
            # 2. ğŸ’¡ ØªÙ†ÙÙŠØ° JavaScript: Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø­Ø§Ø³Ù…Ø©!
            # Ù†Ø¬Ø¹Ù„ Ø§Ù„Ø¨ÙˆØª ÙŠÙ†ØªØ¸Ø± 3 Ø«ÙˆØ§Ù†Ù Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            response.html.render(sleep=3, timeout=30, scrolldown=1) 
            
            # 3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙ†ÙØ° (Rendered Content) Ù…Ø¹ BeautifulSoup
            soup = BeautifulSoup(response.html.html, 'lxml') 
            results = []
            
            # Ù…Ø­Ø¯Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø« (Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ù†Ø© Ø§Ù„ØªÙŠ Ø¹Ù…Ù„Ù†Ø§ Ø¹Ù„ÙŠÙ‡Ø§ Ø³Ø§Ø¨Ù‚Ø§Ù‹)
            book_links = soup.select('a[href*="/book-"]')
            
            unique_books = {}
            for link in book_links:
                book_link_partial = link.get('href')
                
                if book_link_partial and book_link_partial not in unique_books:
                    book_full_link = urljoin(NOOR_BOOK_BASE_URL, book_link_partial)
                    book_title = link.get('title', link.text).strip()
                    
                    if len(book_title) > 5 and book_title.lower() != 'details':
                        unique_books[book_link_partial] = {
                            'title': book_title,
                            'url': book_full_link
                        }

            results = list(unique_books.values())[:MAX_SEARCH_RESULTS]
            
            return results

        except Exception as e:
            logging.error(f"Critical Error during rendering/scraping search: {e}")
            return []
    
    # Ø¯Ø§Ù„Ø© get_download_info Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØºÙŠÙŠØ± ÙƒØ¨ÙŠØ± Ù„Ø£Ù†Ù‡Ø§ ØªØ³ØªÙ‡Ø¯Ù Ø±Ø§Ø¨Ø· ØªØ­Ù…ÙŠÙ„ Ø«Ø§Ø¨Øª
    def get_download_info(self, book_url):
        # ... (Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ù… self.session)
        # Ù…Ø«Ø§Ù„: response = self.session.get(book_url, headers=self.headers, timeout=15)
        # ... (ÙŠØ¬Ø¨ ØªØ¹Ø¯ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… requests.get Ø¥Ù„Ù‰ self.session.get ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø£ÙŠØ¶Ø§Ù‹)
        pass # Ø³ÙŠØªÙ… ÙˆØ¶Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ø¯Ù†Ø§Ù‡

# Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù…Ø­Ø¯Ø« Ù„Ù€ scraper.py (Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ø§Ù‹)
