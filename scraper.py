# scraper.py
import logging
import re
import requests
import time
from config import MAX_SEARCH_RESULTS
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup 

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def __init__(self):
        # Ø±Ø¤ÙˆØ³ Ø«Ø§Ø¨ØªØ© Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ar-EG,ar;q=0.9,en-US;q=0.8,en;q=0.7'
        }

    def search_library(self, query):
        """
        ØªÙØ¬Ø±ÙŠ Ø¨Ø­Ø«Ø§Ù‹ Ø¹Ø§Ù…Ø§Ù‹ ÙˆØ´Ø§Ù…Ù„Ø§Ù‹ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ø¹Ø¨Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø«.
        (Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ§Ø¨ Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ÙŠÙƒÙ† Ù…Ù„Ù PDF Ù…ÙÙ‡Ø±Ø³Ø§Ù‹)
        """
        logging.info(f"Initiating broad search for book page: {query}")
        
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ø¹Ø§Ù…Ø© (Ø§Ø³ØªÙ‡Ø¯Ø§Ù ØµÙØ­Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨)
        search_queries = [
            f"site:noor-book.com {query} ÙƒØªØ§Ø¨", # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙØ­Ø© Ø§Ù„ÙƒØªØ§Ø¨
            f"site:kutubati.com {query} ÙƒØªØ§Ø¨"
        ]
        
        books = []
        
        # 2. ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± Ø£Ø¯Ø§Ø© Google Search
        try:
            # ğŸ’¡ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ø§Ø© google:search Ù‡Ù†Ø§
            search_results = google.search(queries=search_queries)
        except Exception as e:
            logging.error(f"Google Search Tool Failed: {e}")
            return []
            
        
        # 3. ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ¬Ù‡ÙŠØ²Ù‡Ø§
        for result in search_results:
            url = result.url.lower()
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ´ÙŠØ± Ù„ØµÙØ­Ø© ÙƒØªØ§Ø¨ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„
            if 'book' in url or 'download' in url:
                books.append({
                    # ØªÙ†Ø¸ÙŠÙ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    'title': re.sub(r' \| .*', '', result.title).strip(),
                    'url': result.url 
                })
                if len(books) >= MAX_SEARCH_RESULTS:
                    break
        
        return books

    def get_download_info(self, book_url):
        """
        ØªØªØ£ÙƒØ¯ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØªØ¹ÙˆØ¯ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù…Ù„Ù Ø¹Ø¨Ø± ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡.
        """
        logging.info(f"Checking link for direct file: {book_url}")
        
        try:
            # 1. Ù…Ø­Ø§ÙˆÙ„Ø© ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø±Ø§Ø¨Ø· Ø§Ù„Ù†ØªÙŠØ¬Ø©
            response = requests.get(book_url, allow_redirects=True, timeout=15, headers=self.headers)
            final_url = response.url
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙŠØ´ÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù…Ù„Ù
            if final_url.lower().endswith(('.pdf', '.epub')):
                file_ext = '.pdf' if final_url.lower().endswith('.pdf') else '.epub'
                return final_url, file_ext
            
            # 2. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø§ ÙŠØ²Ø§Ù„ ØµÙØ­Ø© ÙˆÙŠØ¨ØŒ Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ÙƒØ´Ø· Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (ÙƒØ­Ù„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ)
            soup = BeautifulSoup(response.content, 'lxml')
            download_button = soup.select_one('a[href*="/download/"], a.btn-download')
            
            if download_button:
                download_link_partial = download_button.get('href')
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… final_url ÙƒÙ€ base url ÙÙŠ Ø­Ø§Ù„ ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© 1
                full_download_link = urljoin(final_url, download_link_partial) 
                
                # ØªØªØ¨Ø¹ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù…Ù„Ù
                final_file_response = requests.get(full_download_link, allow_redirects=True, timeout=30, headers=self.headers)
                final_file_url = final_file_response.url
                
                if final_file_url.lower().endswith(('.pdf', '.epub')):
                    file_ext = '.pdf' if final_file_url.lower().endswith('.pdf') else '.epub'
                    return final_file_url, file_ext

            return None, "link" # Ù„Ù… Ù†Ø¬Ø¯ Ø±Ø§Ø¨Ø· Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±
            
        except Exception as e:
            logging.error(f"Error during link check/redirection: {e}")
            return None, "error"
