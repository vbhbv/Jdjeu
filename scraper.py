# scraper.py
import logging
import re
import requests
import time
from config import MAX_SEARCH_RESULTS
from urllib.parse import quote

# âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ¹ØªÙ…Ø¯ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø© google:search Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ÙŠ
# Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø©ØŒ ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù€ Google Custom Search API Ø£Ùˆ Ø£ÙŠ Ø®Ø¯Ù…Ø© Ø¨Ø­Ø« Ø£Ø®Ø±Ù‰.

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def search_library(self, query):
        """
        ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø­Ø« Ù…ÙˆØ«ÙˆÙ‚ ÙˆÙ…Ø®ØµØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Search Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· PDF Ù…Ø¨Ø§Ø´Ø±Ø©.
        """
        logging.info(f"Initiating powerful filetype search for: {query}")
        
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ù…ÙˆØ¬Ù‡Ø© Ù„Ù…Ù„ÙØ§Øª PDF/EPUB Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
        search_queries = [
            f"site:noor-book.com {query} filetype:pdf OR filetype:epub",
            f"site:kutubati.com {query} filetype:pdf OR filetype:epub"
        ]
        
        books = []
        
        # 2. ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± Ø£Ø¯Ø§Ø© Google Search
        try:
            # ğŸ’¡ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ø§Ø© Ù‡Ù†Ø§ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ©
            search_results = google.search(queries=search_queries)
        except Exception as e:
            logging.error(f"Google Search Tool Failed: {e}")
            return []
            
        
        # 3. ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ¬Ù‡ÙŠØ²Ù‡Ø§
        for result in search_results:
            url = result.url.lower()
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ´ÙŠØ± Ù„Ù…Ù„Ù
            if url.endswith(('.pdf', '.epub')) or ('download' in url and url.endswith(('.php', '.html'))):
                books.append({
                    # ØªÙ†Ø¸ÙŠÙ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    'title': re.sub(r' \| .*', '', result.title).strip(),
                    'url': result.url 
                })
                if len(books) >= MAX_SEARCH_RESULTS:
                    break
        
        return books

    def get_download_info(self, book_url):
        """
        ØªØªØ£ÙƒØ¯ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØªØ¹ÙˆØ¯ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø±Ø§Ø¨Ø· Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±ØŒ
        ÙØ¥Ù†Ù‡Ø§ ØªØ­Ø§ÙˆÙ„ ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù (ØªØ¹Ù…Ù„ ÙƒØ·Ø¨Ù‚Ø© Ø£Ù…Ø§Ù†).
        """
        logging.info(f"Checking link for direct file: {book_url}")
        
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
            response = requests.get(book_url, allow_redirects=True, timeout=15)
            final_url = response.url
            
            if final_url.lower().endswith(('.pdf', '.epub')):
                file_ext = '.pdf' if final_url.lower().endswith('.pdf') else '.epub'
                return final_url, file_ext
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙ†ØªÙ‡Ù Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ù…Ù„ÙØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† Ù†Ø¹ÙˆØ¯ Ø¨Ù‡ ÙƒØ±Ø§Ø¨Ø·
            return book_url, "link"
            
        except Exception as e:
            logging.error(f"Error during link check: {e}")
            return None, "error"
