# scraper.py (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø£ÙƒØ«Ø± ØªØ­ØµÙŠÙ†Ø§Ù‹)
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import logging

from config import NOOR_BOOK_BASE_URL, NOOR_BOOK_SEARCH_URL, MAX_SEARCH_RESULTS

logging.basicConfig(level=logging.INFO)

class LibraryScraper:
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ar-EG,ar;q=0.9,en-US;q=0.8,en;q=0.7' # Ù„Ø¶Ù…Ø§Ù† Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        }

    def search_library(self, query):
        """ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø¯Ø¯Ø§Øª ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· (Path) Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª."""
        encoded_query = quote(query)
        search_url = NOOR_BOOK_SEARCH_URL.format(query=encoded_query)
        
        logging.info(f"Searching: {search_url}")
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'lxml')
            results = []
            
            # ğŸ’¡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¨ØªÙƒØ± ÙˆØ§Ù„Ø­Ø§Ø³Ù…:
            # Ù†Ø³ØªÙ‡Ø¯Ù Ø£ÙŠ Ø±Ø§Ø¨Ø· (a) ÙŠØ­ØªÙˆÙŠ ÙÙŠ Ø®Ø§ØµÙŠØ© href Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± ÙŠØ¨Ø¯Ùˆ ÙƒØµÙØ­Ø© ÙƒØªØ§Ø¨
            # Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø£Ù‚Ù„ Ø¹Ø±Ø¶Ø© Ù„Ù„ØªØºÙŠÙŠØ± Ù…Ù† Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
            book_links = soup.select('a[href*="/book-"]')
            
            if not book_links:
                logging.warning(f"No book links found using a[href*='/book-'] for query: {query}")
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù† (Ù‚Ø¯ ÙŠØ¹Ù…Ù„)
                book_links = soup.select('a[title]') 

            
            # ÙÙ„ØªØ±Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            unique_books = {}
            for link in book_links:
                book_link_partial = link.get('href')
                
                if book_link_partial and book_link_partial not in unique_books:
                    book_full_link = urljoin(NOOR_BOOK_BASE_URL, book_link_partial)
                    
                    # Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø· Ø£Ùˆ Ø®Ø§ØµÙŠØ© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                    book_title = link.get('title', link.text).strip()
                    
                    if len(book_title) > 5: # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø£Ùˆ Ø§Ù„ÙØ§Ø±ØºØ©
                        unique_books[book_link_partial] = {
                            'title': book_title,
                            'url': book_full_link
                        }

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© ÙˆØªØ±ØªÙŠØ¨Ù‡Ø§
            results = list(unique_books.values())[:MAX_SEARCH_RESULTS]
            
            return results

        except Exception as e:
            logging.error(f"Critical Error during scraping search: {e}")
            return []

    def get_download_info(self, book_url):
        # ... (Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ØŒ Ø³Ù†Ø¨Ù‚ÙŠÙ‡Ø§ ÙƒÙ…Ø§ Ù‡ÙŠ)
        logging.info(f"Visiting book page: {book_url}")
        
        try:
            response = requests.get(book_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'lxml')
            
            download_button = soup.select_one(
                'a[href$=".pdf"], '       
                'a[href$=".epub"], '      
                'a[download], '           
                'a.btn-download, '        
                'a[href*="/download/"]'   
            )
            
            if download_button:
                download_link_partial = download_button.get('href')
                download_link = urljoin(NOOR_BOOK_BASE_URL, download_link_partial)
                
                file_ext = '.pdf' if '.pdf' in download_link.lower() else '.epub'
                
                return download_link, file_ext
            
            return None, None

        except Exception as e:
            logging.error(f"Error during download link extraction: {e}")
            return None, None
